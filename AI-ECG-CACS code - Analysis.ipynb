{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd36e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import base64\n",
    "import array\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from torchsummary import summary\n",
    "from tableone import TableOne\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfdf2da",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inference outputs only\n",
    "df_test=pd.read_csv('20240701_test_inference.csv')\n",
    "df_YSH=pd.read_csv('20240701_YSH_inference.csv')\n",
    "df_AUMC=pd.read_csv('20240701_AUMC_inference.csv')\n",
    "df_val=pd.read_csv('20240701_val_inference.csv')\n",
    "\n",
    "label0=[]\n",
    "for i in range(len(df_test)):\n",
    "    if df_test['CACS'][i]>0:\n",
    "        label0.append(1)\n",
    "    else:\n",
    "        label0.append(0)\n",
    "df_test['label0']=label0\n",
    "\n",
    "label0=[]\n",
    "for i in range(len(df_val)):\n",
    "    if df_val['CACS'][i]>0:\n",
    "        label0.append(1)\n",
    "    else:\n",
    "        label0.append(0)\n",
    "df_val['label0']=label0\n",
    "\n",
    "label0=[]\n",
    "for i in range(len(df_YSH)):\n",
    "    if df_YSH['CACS'][i]>0:\n",
    "        label0.append(1)\n",
    "    else:\n",
    "        label0.append(0)\n",
    "df_YSH['label0']=label0\n",
    "\n",
    "label0=[]\n",
    "for i in range(len(df_AUMC)):\n",
    "    if df_AUMC['score_float'][i]>0:\n",
    "        label0.append(1)\n",
    "    else:\n",
    "        label0.append(0)\n",
    "df_AUMC['label0']=label0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889df55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIpredictiontotallist=[]\n",
    "for i in range(25):\n",
    "    AIpredictiontotallist.append('AI_prediction_'+str(i))\n",
    "    \n",
    "df_test['AI_average']=df_test[AIpredictiontotallist].mean(axis=1)\n",
    "df_YSH['AI_average']=df_YSH[AIpredictiontotallist].mean(axis=1)\n",
    "df_AUMC['AI_average']=df_AUMC[AIpredictiontotallist].mean(axis=1)\n",
    "df_val['AI_average']=df_val[AIpredictiontotallist].mean(axis=1)\n",
    "\n",
    "print('==============')\n",
    "print('df val')\n",
    "fpr_val,tpr_val,thresholds=metrics.roc_curve(df_val['label400'],df_val['AI_average'],pos_label=1)\n",
    "AUROC_val=metrics.auc(fpr_val,tpr_val)\n",
    "print(AUROC_val)\n",
    "\n",
    "precision_val, recall_val, _ = metrics.precision_recall_curve(df_val['label400'], df_val['AI_average'], pos_label=1)\n",
    "AUPRC_val = metrics.auc(recall_val, precision_val)\n",
    "print(\"AUPRC_val:\", AUPRC_val)\n",
    "\n",
    "youden_j = tpr_val - fpr_val\n",
    "max_j_index = np.argmax(youden_j)\n",
    "optimal_threshold = thresholds[max_j_index]\n",
    "\n",
    "\n",
    "print('========================')\n",
    "print('df test')\n",
    "print('=====')\n",
    "print('label400')\n",
    "fpr_test,tpr_test,threshold=metrics.roc_curve(df_test['label400'],df_test['AI_average'],pos_label=1)\n",
    "AUROC_test=metrics.auc(fpr_test,tpr_test)\n",
    "print(AUROC_test)\n",
    "\n",
    "precision_test, recall_test, _ = metrics.precision_recall_curve(df_test['label400'], df_test['AI_average'], pos_label=1)\n",
    "AUPRC_test = metrics.auc(recall_test, precision_test)\n",
    "print(\"AUPRC_test:\", AUPRC_test)\n",
    "\n",
    "df_test['predicted'] = (df_test['AI_average'] >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(df_test['label400'], df_test['predicted']).ravel()\n",
    "accuracy = metrics.accuracy_score(df_test['label400'], df_test['predicted'])\n",
    "sensitivity = tp / (tp + fn)  # also known as recall\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)  # positive predictive value (precision)\n",
    "npv = tn / (tn + fn)  # negative predictive value\n",
    "f1_score = metrics.f1_score(df_test['label400'], df_test['predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"PPV (Precision): {ppv}\")\n",
    "print(f\"NPV: {npv}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print('=====')\n",
    "print('label0')\n",
    "fpr_test,tpr_test,threshold=metrics.roc_curve(df_test['label0'],df_test['AI_average'],pos_label=1)\n",
    "AUROC_test=metrics.auc(fpr_test,tpr_test)\n",
    "print(AUROC_test)\n",
    "\n",
    "precision_test, recall_test, _ = metrics.precision_recall_curve(df_test['label0'], df_test['AI_average'], pos_label=1)\n",
    "AUPRC_test = metrics.auc(recall_test, precision_test)\n",
    "print(\"AUPRC_test:\", AUPRC_test)\n",
    "\n",
    "df_test['predicted'] = (df_test['AI_average'] >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(df_test['label0'], df_test['predicted']).ravel()\n",
    "accuracy = metrics.accuracy_score(df_test['label0'], df_test['predicted'])\n",
    "sensitivity = tp / (tp + fn)  # also known as recall\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)  # positive predictive value (precision)\n",
    "npv = tn / (tn + fn)  # negative predictive value\n",
    "f1_score = metrics.f1_score(df_test['label0'], df_test['predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"PPV (Precision): {ppv}\")\n",
    "print(f\"NPV: {npv}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "\n",
    "\n",
    "print('========================')\n",
    "print('df YSH')\n",
    "print('=====')\n",
    "print('label400')\n",
    "fpr_YSH,tpr_YSH,threshold=metrics.roc_curve(df_YSH['label400'],df_YSH['AI_average'],pos_label=1)\n",
    "AUROC_YSH=metrics.auc(fpr_YSH,tpr_YSH)\n",
    "print(AUROC_YSH)\n",
    "\n",
    "precision_YSH, recall_YSH, _ = metrics.precision_recall_curve(df_YSH['label400'], df_YSH['AI_average'], pos_label=1)\n",
    "AUPRC_YSH = metrics.auc(recall_YSH, precision_YSH)\n",
    "print(\"AUPRC_YSH:\", AUPRC_YSH)\n",
    "\n",
    "df_YSH['predicted'] = (df_YSH['AI_average'] >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(df_YSH['label400'], df_YSH['predicted']).ravel()\n",
    "accuracy = metrics.accuracy_score(df_YSH['label400'], df_YSH['predicted'])\n",
    "sensitivity = tp / (tp + fn)  # also known as recall\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)  # positive predictive value (precision)\n",
    "npv = tn / (tn + fn)  # negative predictive value\n",
    "f1_score = metrics.f1_score(df_YSH['label400'], df_YSH['predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"PPV (Precision): {ppv}\")\n",
    "print(f\"NPV: {npv}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "\n",
    "print('=====')\n",
    "print('label0')\n",
    "fpr_YSH,tpr_YSH,threshold=metrics.roc_curve(df_YSH['label0'],df_YSH['AI_average'],pos_label=1)\n",
    "AUROC_YSH=metrics.auc(fpr_YSH,tpr_YSH)\n",
    "print(AUROC_YSH)\n",
    "\n",
    "precision_YSH, recall_YSH, _ = metrics.precision_recall_curve(df_YSH['label0'], df_YSH['AI_average'], pos_label=1)\n",
    "AUPRC_YSH = metrics.auc(recall_YSH, precision_YSH)\n",
    "print(\"AUPRC_YSH:\", AUPRC_YSH)\n",
    "\n",
    "df_YSH['predicted'] = (df_YSH['AI_average'] >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(df_YSH['label0'], df_YSH['predicted']).ravel()\n",
    "accuracy = metrics.accuracy_score(df_YSH['label0'], df_YSH['predicted'])\n",
    "sensitivity = tp / (tp + fn)  # also known as recall\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)  # positive predictive value (precision)\n",
    "npv = tn / (tn + fn)  # negative predictive value\n",
    "f1_score = metrics.f1_score(df_YSH['label0'], df_YSH['predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"PPV (Precision): {ppv}\")\n",
    "print(f\"NPV: {npv}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "print('========================')\n",
    "print('df AUMC')\n",
    "print('=====')\n",
    "print('label400')\n",
    "fpr_AUMC,tpr_AUMC,threshold=metrics.roc_curve(df_AUMC['label400'],df_AUMC['AI_average'],pos_label=1)\n",
    "AUROC_AUMC=metrics.auc(fpr_AUMC,tpr_AUMC)\n",
    "print(AUROC_AUMC)\n",
    "\n",
    "precision_AUMC, recall_AUMC, _ = metrics.precision_recall_curve(df_AUMC['label400'], df_AUMC['AI_average'], pos_label=1)\n",
    "AUPRC_AUMC = metrics.auc(recall_AUMC, precision_AUMC)\n",
    "print(\"AUPRC_AUMC:\", AUPRC_AUMC)\n",
    "\n",
    "df_AUMC['predicted'] = (df_AUMC['AI_average'] >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(df_AUMC['label400'], df_AUMC['predicted']).ravel()\n",
    "accuracy = metrics.accuracy_score(df_AUMC['label400'], df_AUMC['predicted'])\n",
    "sensitivity = tp / (tp + fn)  # also known as recall\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)  # positive predictive value (precision)\n",
    "npv = tn / (tn + fn)  # negative predictive value\n",
    "f1_score = metrics.f1_score(df_AUMC['label400'], df_AUMC['predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"PPV (Precision): {ppv}\")\n",
    "print(f\"NPV: {npv}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "print('=====')\n",
    "print('label0')\n",
    "fpr_AUMC,tpr_AUMC,threshold=metrics.roc_curve(df_AUMC['label0'],df_AUMC['AI_average'],pos_label=1)\n",
    "AUROC_AUMC=metrics.auc(fpr_AUMC,tpr_AUMC)\n",
    "print(AUROC_AUMC)\n",
    "\n",
    "precision_AUMC, recall_AUMC, _ = metrics.precision_recall_curve(df_AUMC['label0'], df_AUMC['AI_average'], pos_label=1)\n",
    "AUPRC_AUMC = metrics.auc(recall_AUMC, precision_AUMC)\n",
    "print(\"AUPRC_AUMC:\", AUPRC_AUMC)\n",
    "\n",
    "df_AUMC['predicted'] = (df_AUMC['AI_average'] >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(df_AUMC['label0'], df_AUMC['predicted']).ravel()\n",
    "accuracy = metrics.accuracy_score(df_AUMC['label0'], df_AUMC['predicted'])\n",
    "sensitivity = tp / (tp + fn)  # also known as recall\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)  # positive predictive value (precision)\n",
    "npv = tn / (tn + fn)  # negative predictive value\n",
    "f1_score = metrics.f1_score(df_AUMC['label0'], df_AUMC['predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"PPV (Precision): {ppv}\")\n",
    "print(f\"NPV: {npv}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROCs=[]\n",
    "for i in range(25):\n",
    "    fpr_test,tpr_test,threshold=metrics.roc_curve(df_test['label0'],df_test['AI_prediction_'+str(i)],pos_label=1)\n",
    "    AUROC_test=metrics.auc(fpr_test,tpr_test)\n",
    "    AUROCs.append(AUROC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b978c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(AUROCs))\n",
    "print(np.std(AUROCs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROCs=[]\n",
    "for i in range(25):\n",
    "    fpr_test,tpr_test,threshold=metrics.roc_curve(df_test['label400'],df_test['AI_prediction_'+str(i)],pos_label=1)\n",
    "    AUROC_test=metrics.auc(fpr_test,tpr_test)\n",
    "    AUROCs.append(AUROC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d40945",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(AUROCs))\n",
    "print(np.std(AUROCs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5224c4",
   "metadata": {},
   "source": [
    "## ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ba0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_test,tpr_test,threshold=metrics.roc_curve(df_test['label400'],df_test['AI_average'],pos_label=1)\n",
    "AUROC_test=metrics.auc(fpr_test,tpr_test)\n",
    "\n",
    "fpr_YSH,tpr_YSH,threshold=metrics.roc_curve(df_YSH['label400'],df_YSH['AI_average'],pos_label=1)\n",
    "AUROC_YSH=metrics.auc(fpr_YSH,tpr_YSH)\n",
    "print(AUROC_YSH)\n",
    "\n",
    "fpr_AUMC,tpr_AUMC,threshold=metrics.roc_curve(df_AUMC['label400'],df_AUMC['AI_average'],pos_label=1)\n",
    "AUROC_AUMC=metrics.auc(fpr_AUMC,tpr_AUMC)\n",
    "print(AUROC_AUMC)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim([-0.00, 1.00])\n",
    "plt.ylim([-0.00, 1.00])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='grey',alpha=.8)\n",
    "\n",
    "\n",
    "plt.plot(fpr_test, tpr_test, color='blue',label='Test AUROC %0.3f' % (AUROC_test) ,lw=1, alpha=1)\n",
    "plt.plot(fpr_YSH, tpr_YSH, color='red',label=r'External validation (YSH) AUROC %0.3f' % (AUROC_YSH) ,lw=1, alpha=1)\n",
    "plt.plot(fpr_AUMC, tpr_AUMC, color='green',label=r'External validation (AUMC) AUROC %0.3f' % (AUROC_AUMC) ,lw=1, alpha=1)\n",
    "plt.xlabel('1-Specificity', fontsize=17)\n",
    "plt.ylabel('Sensitivity', fontsize=17)\n",
    "plt.title('ROC curves', fontsize=19)\n",
    "#plt.legend(loc=\"lower right\", fontsize=29)\n",
    "plt.xticks(np.arange(0.2, 1.2, step=0.2))\n",
    "plt.yticks(np.arange(0, 1.2, step=0.2))\n",
    "plt.xticks(fontsize =15)\n",
    "plt.yticks(fontsize =15)\n",
    "leg=plt.legend(loc=\"lower right\", fontsize=17)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(2.0)\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('figures/20240702_ROC400.png',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_test,tpr_test,threshold=metrics.roc_curve(df_test['label0'],df_test['AI_average'],pos_label=1)\n",
    "AUROC_test=metrics.auc(fpr_test,tpr_test)\n",
    "\n",
    "fpr_YSH,tpr_YSH,threshold=metrics.roc_curve(df_YSH['label0'],df_YSH['AI_average'],pos_label=1)\n",
    "AUROC_YSH=metrics.auc(fpr_YSH,tpr_YSH)\n",
    "print(AUROC_YSH)\n",
    "\n",
    "fpr_AUMC,tpr_AUMC,threshold=metrics.roc_curve(df_AUMC['label0'],df_AUMC['AI_average'],pos_label=1)\n",
    "AUROC_AUMC=metrics.auc(fpr_AUMC,tpr_AUMC)\n",
    "print(AUROC_AUMC)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim([-0.00, 1.00])\n",
    "plt.ylim([-0.00, 1.00])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='grey',alpha=.8)\n",
    "\n",
    "\n",
    "plt.plot(fpr_test, tpr_test, color='blue',label='Test AUROC %0.3f' % (AUROC_test) ,lw=1, alpha=1)\n",
    "plt.plot(fpr_YSH, tpr_YSH, color='red',label=r'External validation (YSH) AUROC %0.3f' % (AUROC_YSH) ,lw=1, alpha=1)\n",
    "plt.plot(fpr_AUMC, tpr_AUMC, color='green',label=r'External validation (AUMC) AUROC %0.3f' % (AUROC_AUMC) ,lw=1, alpha=1)\n",
    "plt.xlabel('1-Specificity', fontsize=17)\n",
    "plt.ylabel('Sensitivity', fontsize=17)\n",
    "plt.title('ROC curves', fontsize=19)\n",
    "#plt.legend(loc=\"lower right\", fontsize=29)\n",
    "plt.xticks(np.arange(0.2, 1.2, step=0.2))\n",
    "plt.yticks(np.arange(0, 1.2, step=0.2))\n",
    "plt.xticks(fontsize =15)\n",
    "plt.yticks(fontsize =15)\n",
    "leg=plt.legend(loc=\"lower right\", fontsize=17)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(2.0)\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('figures/20240702_ROC_CAC0.png',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303159d5",
   "metadata": {},
   "source": [
    "## PR curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_test, recall_test, _ = metrics.precision_recall_curve(df_test['label400'], df_test['AI_average'], pos_label=1)\n",
    "AUPRC_test = metrics.auc(recall_test, precision_test)\n",
    "print(\"AUPRC_test:\", AUPRC_test)\n",
    "\n",
    "precision_YSH, recall_YSH, _ = metrics.precision_recall_curve(df_YSH['label400'], df_YSH['AI_average'], pos_label=1)\n",
    "AUPRC_YSH = metrics.auc(recall_YSH, precision_YSH)\n",
    "print(\"AUPRC_YSH:\", AUPRC_YSH)\n",
    "\n",
    "precision_AUMC, recall_AUMC, _ = metrics.precision_recall_curve(df_AUMC['label400'], df_AUMC['AI_average'], pos_label=1)\n",
    "AUPRC_AUMC = metrics.auc(recall_AUMC, precision_AUMC)\n",
    "print(\"AUPRC_AUMC:\", AUPRC_AUMC)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim([-0.00, 1.00])\n",
    "plt.ylim([-0.00, 1.00])\n",
    "\n",
    "plt.plot(recall_test, precision_test, color='blue', label='Test AUPRC %0.3f' % (AUPRC_test), lw=1, alpha=1)\n",
    "plt.plot(recall_YSH, precision_YSH, color='red', label=r'External validation (YSH) AUPRC %0.3f' % (AUPRC_YSH), lw=1, alpha=1)\n",
    "plt.plot(recall_AUMC, precision_AUMC, color='green', label=r'External validation (AUMC) AUPRC %0.3f' % (AUPRC_AUMC), lw=1, alpha=1)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=17)\n",
    "plt.ylabel('Precision', fontsize=17)\n",
    "plt.title('PR curves', fontsize=19)\n",
    "plt.xticks(np.arange(0.2, 1.2, step=0.2))\n",
    "plt.yticks(np.arange(0, 1.2, step=0.2))\n",
    "plt.xticks(fontsize =15)\n",
    "plt.yticks(fontsize =15)\n",
    "leg=plt.legend(loc=\"upper right\", fontsize=17)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(2.0)\n",
    "\n",
    "plt.savefig('figures/20240702_PR_CAC400.png',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_test, recall_test, _ = metrics.precision_recall_curve(df_test['label0'], df_test['AI_average'], pos_label=1)\n",
    "AUPRC_test = metrics.auc(recall_test, precision_test)\n",
    "print(\"AUPRC_test:\", AUPRC_test)\n",
    "\n",
    "precision_YSH, recall_YSH, _ = metrics.precision_recall_curve(df_YSH['label0'], df_YSH['AI_average'], pos_label=1)\n",
    "AUPRC_YSH = metrics.auc(recall_YSH, precision_YSH)\n",
    "print(\"AUPRC_YSH:\", AUPRC_YSH)\n",
    "\n",
    "precision_AUMC, recall_AUMC, _ = metrics.precision_recall_curve(df_AUMC['label0'], df_AUMC['AI_average'], pos_label=1)\n",
    "AUPRC_AUMC = metrics.auc(recall_AUMC, precision_AUMC)\n",
    "print(\"AUPRC_AUMC:\", AUPRC_AUMC)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim([-0.00, 1.00])\n",
    "plt.ylim([-0.00, 1.00])\n",
    "\n",
    "plt.plot(recall_test, precision_test, color='blue', label='Test AUPRC %0.3f' % (AUPRC_test), lw=1, alpha=1)\n",
    "plt.plot(recall_YSH, precision_YSH, color='red', label=r'External validation (YSH) AUPRC %0.3f' % (AUPRC_YSH), lw=1, alpha=1)\n",
    "plt.plot(recall_AUMC, precision_AUMC, color='green', label=r'External validation (AUMC) AUPRC %0.3f' % (AUPRC_AUMC), lw=1, alpha=1)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=17)\n",
    "plt.ylabel('Precision', fontsize=17)\n",
    "plt.title('PR curves', fontsize=19)\n",
    "plt.xticks(np.arange(0.2, 1.2, step=0.2))\n",
    "plt.yticks(np.arange(0, 1.2, step=0.2))\n",
    "plt.xticks(fontsize =15)\n",
    "plt.yticks(fontsize =15)\n",
    "leg=plt.legend(loc=\"upper right\", fontsize=17)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(2.0)\n",
    "\n",
    "plt.savefig('figures/20240702_PR_CAC0.png',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1f1bd",
   "metadata": {},
   "source": [
    "## Boxplot of AI average by CACS group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-0.1, 0.1, 100, 400, 1000, float('inf')]  # -0.1 to include 0 as a separate bin\n",
    "labels = ['0', '0-100', '100-400', '400-1000', '1000+']\n",
    "\n",
    "# Create a new column for the CACS group\n",
    "df_test['CACS_group'] = pd.cut(df_test['CACS'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Create the boxplot with customized outlier dot size, color, and transparency\n",
    "plt.figure(figsize=(20, 12))\n",
    "boxplot = df_test.boxplot(column='AI_average', by='CACS_group', grid=False, \n",
    "                     flierprops=dict(marker='o', color='grey', alpha=0.6, markersize=0.6))\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(' ', fontsize=13)\n",
    "plt.suptitle('') \n",
    "plt.xlabel('CAC score group', fontsize=11)\n",
    "plt.ylabel('AI-enabled ECG model prediction score', fontsize=11)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('figures/20240702_Boxplot_of_AI_average_by_CACS_group.png',transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262d664",
   "metadata": {},
   "source": [
    "# PCE calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AHA_calculator(Race, Sex, age, total_chol, hdl_chol, sbp, on_hypertension_meds, smoker, diabetes):\n",
    "    \n",
    "    # Selecting coefficients based on race and sex\n",
    "    if Race == 1 and Sex == 0:\n",
    "        C_Age = -29.799\n",
    "        C_Sq_Age = 4.884\n",
    "        C_Total_Chol = 13.54\n",
    "        C_Age_Total_Chol = -3.114\n",
    "        C_HDL_Chol = -13.578\n",
    "        C_Age_HDL_Chol = 3.149\n",
    "        C_On_HypertensionMeds = 2.019\n",
    "        C_Age_On_HypertensionMeds = 0\n",
    "        C_Off_HypertensionMeds = 1.957\n",
    "        C_Age_Off_HypertensionMeds = 0\n",
    "        C_Smoker = 7.574\n",
    "        C_Age_Smoker = -1.665\n",
    "        C_Diabetes = 0.661\n",
    "        S10 = 0.9665\n",
    "        Mean_Terms = -29.18\n",
    "    \n",
    "    elif Race == 1 and Sex == 1:\n",
    "        C_Age = 12.344\n",
    "        C_Sq_Age = 0\n",
    "        C_Total_Chol = 11.853\n",
    "        C_Age_Total_Chol = -2.664\n",
    "        C_HDL_Chol = -7.99\n",
    "        C_Age_HDL_Chol = 1.769\n",
    "        C_On_HypertensionMeds = 1.797\n",
    "        C_Age_On_HypertensionMeds = 0\n",
    "        C_Off_HypertensionMeds = 1.764\n",
    "        C_Age_Off_HypertensionMeds = 0\n",
    "        C_Smoker = 7.837\n",
    "        C_Age_Smoker = -1.795\n",
    "        C_Diabetes = 0.658\n",
    "        S10 = 0.9144\n",
    "        Mean_Terms = 61.18\n",
    "    \n",
    "    elif Race == 0 and Sex == 0:\n",
    "        C_Age = 17.114\n",
    "        C_Sq_Age = 0\n",
    "        C_Total_Chol = 0.94\n",
    "        C_Age_Total_Chol = 0\n",
    "        C_HDL_Chol = -18.92\n",
    "        C_Age_HDL_Chol = 4.475\n",
    "        C_On_HypertensionMeds = 29.291\n",
    "        C_Age_On_HypertensionMeds = -6.432\n",
    "        C_Off_HypertensionMeds = 27.82\n",
    "        C_Age_Off_HypertensionMeds = -6.087\n",
    "        C_Smoker = 0.691\n",
    "        C_Age_Smoker = 0\n",
    "        C_Diabetes = 0.874\n",
    "        S10 = 0.9533\n",
    "        Mean_Terms = 86.61\n",
    "    \n",
    "    elif Race == 0 and Sex == 1:\n",
    "        C_Age = 2.469\n",
    "        C_Sq_Age = 0\n",
    "        C_Total_Chol = 0.302\n",
    "        C_Age_Total_Chol = 0\n",
    "        C_HDL_Chol = -0.307\n",
    "        C_Age_HDL_Chol = 0\n",
    "        C_On_HypertensionMeds = 1.916\n",
    "        C_Age_On_HypertensionMeds = 0\n",
    "        C_Off_HypertensionMeds = 1.809\n",
    "        C_Age_Off_HypertensionMeds = 0\n",
    "        C_Smoker = 0.549\n",
    "        C_Age_Smoker = 0\n",
    "        C_Diabetes = 0.645\n",
    "        S10 = 0.8954\n",
    "        Mean_Terms = 19.54\n",
    "    else:\n",
    "        print('wrong')\n",
    "        \n",
    "    #risk calculation(%)\n",
    "    terms = (C_Age * math.log(age)) + \\\n",
    "        (C_Sq_Age * math.pow(math.log(age), 2)) + \\\n",
    "        (C_Total_Chol * math.log(total_chol)) + \\\n",
    "        (C_Age_Total_Chol * math.log(age) * math.log(total_chol)) + \\\n",
    "        (C_HDL_Chol * math.log(hdl_chol)) + \\\n",
    "        (C_Age_HDL_Chol * math.log(age) * math.log(hdl_chol)) + \\\n",
    "        (on_hypertension_meds * C_On_HypertensionMeds * math.log(sbp)) + \\\n",
    "        (on_hypertension_meds * C_Age_On_HypertensionMeds * math.log(age) * math.log(sbp)) + \\\n",
    "        ((1 - on_hypertension_meds) * C_Off_HypertensionMeds * math.log(sbp)) + \\\n",
    "        ((1 - on_hypertension_meds) * C_Age_Off_HypertensionMeds * math.log(age) * math.log(sbp)) + \\\n",
    "        (C_Smoker * smoker) + \\\n",
    "        (C_Age_Smoker * math.log(age) * smoker) + \\\n",
    "        (C_Diabetes * diabetes)\n",
    "\n",
    "    risk = 100 * (1 - math.pow(S10,(math.exp(terms - Mean_Terms))))\n",
    "    risk=round(risk, 1)\n",
    "    return risk\n",
    "\n",
    "\n",
    "sexint=[]\n",
    "for i in range(len(df_test)):\n",
    "    if df_test['sex'][i]=='M':\n",
    "        sexint.append(1)\n",
    "    else:\n",
    "        sexint.append(0)\n",
    "df_test['sexint']=sexint\n",
    "\n",
    "import math\n",
    "PCE=[]\n",
    "for i in range(len(df_test)):\n",
    "    score=AHA_calculator(1,df_test['sexint'][i],df_test['year_difference'][i],df_test['Chol'][i],df_test['HDL'][i],df_test['SBP'][i],df_test['HTN'][i],df_test['smoking'][i],df_test['DM'][i]   )\n",
    "    PCE.append(score)\n",
    "    if np.isnan(score)==True:\n",
    "        print(df.iloc[i])\n",
    "df_test['PCE']=PCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb78b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_PCE=df_test.dropna(subset=['PCE'])\n",
    "df_test_PCE.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCEcat=[]\n",
    "for i in range(len(df_test_PCE)):\n",
    "    if df_test_PCE['PCE'][i]<7.5:\n",
    "        PCEcat.append(1)\n",
    "    elif df_test_PCE['PCE'][i]<20:\n",
    "        PCEcat.append(2)\n",
    "    else:\n",
    "        PCEcat.append(3)\n",
    "df_test_PCE['PCEcat']=PCEcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec618362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_PCE_low=df_test_PCE[df_test_PCE['PCE']<7.5]\n",
    "df_test_PCE_mod=df_test_PCE[df_test_PCE['PCE']>=7.5]\n",
    "df_test_PCE_mod=df_test_PCE_mod[df_test_PCE_mod['PCE']<20]\n",
    "df_test_PCE_hig=df_test_PCE[df_test_PCE['PCE']>=20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47122b5",
   "metadata": {},
   "source": [
    "# performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thres1= #enter threshold for low vs mod/high risk\n",
    "thres2= #enter threshold for low/mod vs high risk\n",
    "\n",
    "columnslist=[]\n",
    "columnslist.append('CAClevel')\n",
    "columnslist.append('institution')\n",
    "columnslist.append('threshold')\n",
    "columnslist.append('AUROC')\n",
    "columnslist.append('AUPRC')\n",
    "columnslist.append('accuracy')\n",
    "columnslist.append('sensitivity')\n",
    "columnslist.append('specificity')\n",
    "columnslist.append('PPV')\n",
    "columnslist.append('NPV')\n",
    "columnslist.append('F1score')\n",
    "performance_df=pd.DataFrame(columns=columnslist)\n",
    "\n",
    "\n",
    "for labelname in ['label0','label400']:\n",
    "    for institutiontemp in ['val','SH','YSH','AUMC','df_test_PCE_low','df_test_PCE_mod','df_test_PCE_hig']:\n",
    "        if institutiontemp=='SH':\n",
    "            df=df_test.copy()\n",
    "        elif institutiontemp=='YSH':\n",
    "            df=df_YSH.copy()        \n",
    "        elif institutiontemp=='val':\n",
    "            df=df_val.copy() \n",
    "        elif institutiontemp=='df_test_PCE_low':\n",
    "            df=df_test_PCE_low.copy() \n",
    "        elif institutiontemp=='df_test_PCE_mod':\n",
    "            df=df_test_PCE_mod.copy() \n",
    "        elif institutiontemp=='df_test_PCE_hig':\n",
    "            df=df_test_PCE_hig.copy() \n",
    "        else:\n",
    "            df=df_AUMC.copy()\n",
    "        \n",
    "        for thresholdtemp in [thres1,thres2]:\n",
    "            templist=[]\n",
    "            templist.append(labelname)\n",
    "            templist.append(institutiontemp)\n",
    "            templist.append(thresholdtemp)\n",
    "            \n",
    "            fpr,tpr,threshold=metrics.roc_curve(df[labelname],df['AI_average'],pos_label=1)\n",
    "            AUROC=metrics.auc(fpr,tpr)\n",
    "            precision, recall, _ = metrics.precision_recall_curve(df[labelname], df['AI_average'], pos_label=1)\n",
    "            AUPRC = metrics.auc(recall, precision)\n",
    "\n",
    "            df['predicted'] = (df['AI_average'] >= thresholdtemp).astype(int)\n",
    "            tn, fp, fn, tp = metrics.confusion_matrix(df[labelname], df['predicted']).ravel()\n",
    "            accuracy = metrics.accuracy_score(df[labelname], df['predicted'])\n",
    "            sensitivity = tp / (tp + fn)  # also known as recall\n",
    "            specificity = tn / (tn + fp)\n",
    "            ppv = tp / (tp + fp)  # positive predictive value (precision)\n",
    "            npv = tn / (tn + fn)  # negative predictive value\n",
    "            f1_score = metrics.f1_score(df[labelname], df['predicted'])\n",
    "\n",
    "            templist.append(AUROC)\n",
    "            templist.append(AUPRC)\n",
    "            templist.append(accuracy)\n",
    "            templist.append(sensitivity)\n",
    "            templist.append(specificity)\n",
    "            templist.append(ppv)\n",
    "            templist.append(npv)\n",
    "            templist.append(f1_score)\n",
    "            performance_df.loc[len(performance_df)]=templist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e30aae",
   "metadata": {},
   "source": [
    "## Survival analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateuntil=365*9\n",
    "cutlength=90\n",
    "\n",
    "df=pd.read_csv('20240701_survivaldata.csv',encoding='cp949')\n",
    "\n",
    "df['checkup_date'] = pd.to_datetime(df['checkup_date'])\n",
    "df['date2'] = pd.to_datetime(df['date2'])\n",
    "df.loc[df['date2'] > pd.Timestamp('2022-12-31'), 'date2'] = pd.NaT\n",
    "\n",
    "df['status'] = df['date2'].notnull().astype(int)\n",
    "df=df[~(df['checkup_date']  >= df['date2'])]\n",
    "df['day_difference'] = df.apply(lambda row: (row['date2'] - row['checkup_date']).days if row['status'] == 1 else (pd.Timestamp('2022-12-31') - row['checkup_date']).days, axis=1)\n",
    "\n",
    "df = df[(df['day_difference'] >= cutlength) | df['day_difference'].isna()]\n",
    "df = df.reset_index(drop=True)\n",
    "df['day_difference'] = df['day_difference'] - cutlength\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df['status'][i]==1 and df['day_difference'][i]>=dateuntil:\n",
    "        df['status'][i]=0\n",
    "        df['day_difference'][i]=dateuntil\n",
    "    if df['status'][i]==0 and df['day_difference'][i]>=dateuntil:\n",
    "        df['day_difference'][i]=dateuntil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already loaded and the columns are prepared as per the previous code\n",
    "# Calculate person-time in years\n",
    "df['person_years'] = df['day_difference'] / 365.25\n",
    "\n",
    "# Calculate total number of events\n",
    "total_events = df['status'].sum()\n",
    "\n",
    "# Calculate total person-time\n",
    "total_person_time = df['person_years'].sum()\n",
    "\n",
    "# Calculate incidence rate per 1000 person-years\n",
    "incidence_rate = (total_events / total_person_time) * 1000\n",
    "\n",
    "print(f\"Incidence rate per 1000 person-years: {incidence_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e35b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df[df['PCEcat']==1]\n",
    "print(len(df2))\n",
    "df2['person_years'] = df2['day_difference'] / 365.25\n",
    "total_events = df2['status'].sum()\n",
    "total_person_time = df2['person_years'].sum()\n",
    "incidence_rate = (total_events / total_person_time) * 10\n",
    "print(f\"Incidence rate per person-10years: {incidence_rate}\")\n",
    "\n",
    "df2=df[df['PCEcat']==2]\n",
    "print(len(df2))\n",
    "df2['person_years'] = df2['day_difference'] / 365.25\n",
    "total_events = df2['status'].sum()\n",
    "total_person_time = df2['person_years'].sum()\n",
    "incidence_rate = (total_events / total_person_time) * 10\n",
    "print(f\"Incidence rate per person-10years: {incidence_rate}\")\n",
    "\n",
    "df2=df[df['PCEcat']==3]\n",
    "print(len(df2))\n",
    "df2['person_years'] = df2['day_difference'] / 365.25\n",
    "total_events = df2['status'].sum()\n",
    "total_person_time = df2['person_years'].sum()\n",
    "incidence_rate = (total_events / total_person_time) * 10\n",
    "print(f\"Incidence rate per person-10years: {incidence_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile(df['person_years'],25))\n",
    "print(np.percentile(df['person_years'],50))\n",
    "print(np.percentile(df['person_years'],75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f816b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIcat=[]\n",
    "for i in range(len(df)):\n",
    "    if df['AI_prediction_average'][i]>=thres2:\n",
    "        AIcat.append(3)\n",
    "    elif df['AI_prediction_average'][i]>=thres1:\n",
    "        AIcat.append(2)\n",
    "    else:\n",
    "        AIcat.append(1)\n",
    "df['AIcat']=AIcat\n",
    "\n",
    "AIcatnum=1\n",
    "df2=df[df['AIcat']==AIcatnum]\n",
    "df2['person_years'] = df2['day_difference'] / 365.25\n",
    "total_events = df2['status'].sum()\n",
    "total_person_time = df2['person_years'].sum()\n",
    "incidence_rate = (total_events / total_person_time) * 10\n",
    "print(f\"total Incidence rate per person-10years: {incidence_rate}\")\n",
    "print(total_events)\n",
    "print(total_person_time)\n",
    "\n",
    "AIcatnum=2\n",
    "df2=df[df['AIcat']==AIcatnum]\n",
    "df2['person_years'] = df2['day_difference'] / 365.25\n",
    "total_events = df2['status'].sum()\n",
    "total_person_time = df2['person_years'].sum()\n",
    "incidence_rate = (total_events / total_person_time) * 10\n",
    "print(f\"total Incidence rate per person-10years: {incidence_rate}\")\n",
    "print(total_events)\n",
    "print(total_person_time)\n",
    "\n",
    "AIcatnum=3\n",
    "df2=df[df['AIcat']==AIcatnum]\n",
    "df2['person_years'] = df2['day_difference'] / 365.25\n",
    "total_events = df2['status'].sum()\n",
    "total_person_time = df2['person_years'].sum()\n",
    "incidence_rate = (total_events / total_person_time) * 10\n",
    "print(f\"total Incidence rate per person-10years: {incidence_rate}\")\n",
    "print(total_events)\n",
    "print(total_person_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f271364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmcurve(df):\n",
    "    TopTertile = df[df['AI_prediction_average'] >thres2]\n",
    "    MiddleTertile= df[df['AI_prediction_average'] <=thres2]\n",
    "    MiddleTertile= MiddleTertile[MiddleTertile['AI_prediction_average'] >thres1]\n",
    "    BottomTertile = df[df['AI_prediction_average'] <=thres1]\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    kmf.fit(TopTertile['day_difference'], TopTertile['status'])\n",
    "    ci_lower = 1 - kmf.confidence_interval_['KM_estimate_upper_0.95']\n",
    "    ci_upper = 1 - kmf.confidence_interval_['KM_estimate_lower_0.95']\n",
    "    plt.fill_between(kmf.survival_function_.index, ci_lower, ci_upper, color='red',alpha=0.3)\n",
    "    plt.step(kmf.survival_function_.index, 1 - kmf.survival_function_, color='red',label='High risk (AI)')\n",
    "\n",
    "    kmf.fit(MiddleTertile['day_difference'], MiddleTertile['status'])\n",
    "    ci_lower = 1 - kmf.confidence_interval_['KM_estimate_upper_0.95']\n",
    "    ci_upper = 1 - kmf.confidence_interval_['KM_estimate_lower_0.95']\n",
    "    plt.fill_between(kmf.survival_function_.index, ci_lower, ci_upper, color='green',alpha=0.3)\n",
    "    plt.step(kmf.survival_function_.index, 1 - kmf.survival_function_, color='green',label='Moderate risk (AI)')\n",
    "\n",
    "\n",
    "    kmf.fit(BottomTertile['day_difference'], BottomTertile['status'])\n",
    "    ci_lower = 1 - kmf.confidence_interval_['KM_estimate_upper_0.95']\n",
    "    ci_upper = 1 - kmf.confidence_interval_['KM_estimate_lower_0.95']\n",
    "    plt.fill_between(kmf.survival_function_.index, ci_lower, ci_upper, color='blue',alpha=0.3)\n",
    "    plt.step(kmf.survival_function_.index, 1 - kmf.survival_function_, color='blue',label='Low risk (AI)')\n",
    "\n",
    "    plt.xlim(0, dateuntil)  # Set x-axis range\n",
    "    plt.ylim([0, 0.048])  # Adjust the limits as per your data\n",
    "\n",
    "    xlabel_size = 22  # Adjust as needed\n",
    "    ylabel_size = 22  # Adjust as needed\n",
    "    ticks_size = 20   # Adjust as needed\n",
    "    title_size = 23   # Adjust as needed\n",
    "    legend_size = 18  # Adjust as needed\n",
    "\n",
    "    plt.xlabel('Time (days)', fontsize=xlabel_size)\n",
    "    plt.ylabel('Cumulative Incidence', fontsize=ylabel_size)\n",
    "    plt.xticks(np.arange(0, dateuntil, step=500), fontsize=ticks_size)  # Adjust step as per your data\n",
    "    plt.yticks(np.arange(0, 0.048, step=0.005), fontsize=ticks_size)\n",
    "\n",
    "    leg=plt.legend(loc='upper left', fontsize=legend_size)\n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(2.5)\n",
    "\n",
    "\n",
    "    plt.savefig('figures/20240709_survival_tertile.png',transparent=True)\n",
    "\n",
    "    # Perform the multivariate logrank test\n",
    "    T = pd.concat([BottomTertile['day_difference'], MiddleTertile['day_difference'], TopTertile['day_difference']])\n",
    "    E = pd.concat([BottomTertile['status'], MiddleTertile['status'], TopTertile['status']])\n",
    "    groups = ['Bottom'] * len(BottomTertile) + ['Middle'] * len(MiddleTertile) + ['Top'] * len(TopTertile)\n",
    "\n",
    "    result = multivariate_logrank_test(T, groups, E)\n",
    "    print(result.print_summary())\n",
    "    print(result.p_value)\n",
    "\n",
    "\n",
    "    T1=BottomTertile['day_difference']\n",
    "    T2=MiddleTertile['day_difference']\n",
    "    E1=BottomTertile['status']\n",
    "    E2=MiddleTertile['status']\n",
    "    logrankresults=logrank_test(T1,T2,event_observed_A=E1,event_observed_B=E2)\n",
    "    print(logrankresults.print_summary())\n",
    "    print(logrankresults.p_value)\n",
    "\n",
    "    T1=MiddleTertile['day_difference']\n",
    "    T2=TopTertile['day_difference']\n",
    "    E1=MiddleTertile['status']\n",
    "    E2=TopTertile['status']\n",
    "    logrankresults=logrank_test(T1,T2,event_observed_A=E1,event_observed_B=E2)\n",
    "    print(logrankresults.print_summary())\n",
    "    print(logrankresults.p_value)\n",
    "\n",
    "    df['person_years'] = df['day_difference'] / 365.25\n",
    "    total_events = df['status'].sum()\n",
    "    total_person_time = df['person_years'].sum()\n",
    "    incidence_rate = (total_events / total_person_time) * 10\n",
    "    print(f\"total Incidence rate per person-10years: {incidence_rate}\")\n",
    "\n",
    "    TopTertile['person_years'] = TopTertile['day_difference'] / 365.25\n",
    "    total_events = TopTertile['status'].sum()\n",
    "    total_person_time = TopTertile['person_years'].sum()\n",
    "    incidence_rate = (total_events / total_person_time) * 10\n",
    "    print(f\"Top Incidence rate per person-10years: {incidence_rate}\")\n",
    "    print(total_events)\n",
    "    print(total_person_time)\n",
    "\n",
    "    MiddleTertile['person_years'] = MiddleTertile['day_difference'] / 365.25\n",
    "    total_events = MiddleTertile['status'].sum()\n",
    "    total_person_time = MiddleTertile['person_years'].sum()\n",
    "    incidence_rate = (total_events / total_person_time) * 10\n",
    "    print(f\"Middle Incidence rate per person-10years: {incidence_rate}\")\n",
    "    print(total_events)\n",
    "    print(total_person_time)\n",
    "\n",
    "    BottomTertile['person_years'] = BottomTertile['day_difference'] / 365.25\n",
    "    total_events = BottomTertile['status'].sum()\n",
    "    total_person_time = BottomTertile['person_years'].sum()\n",
    "    incidence_rate = (total_events / total_person_time) * 10\n",
    "    print(f\"Bottom Incidence rate per person-10years: {incidence_rate}\")\n",
    "    print(total_events)\n",
    "    print(total_person_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCEcatnum=1\n",
    "df2=df[df['PCEcat']==PCEcatnum]\n",
    "kmcurve(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0641c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCEcatnum=2\n",
    "df2=df[df['PCEcat']==PCEcatnum]\n",
    "kmcurve(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65dcc62",
   "metadata": {},
   "source": [
    "# NRI, c-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b53b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df.copy()\n",
    "for statusname in ['status']:\n",
    "    print('============',statusname,'============')\n",
    "    NRI=0\n",
    "    for i in [0,1]:\n",
    "        if i==0:\n",
    "            print('neg')\n",
    "        else:\n",
    "            print('pos')\n",
    "        dftemp=df_final[df_final[statusname]==i]\n",
    "        print('total',len(dftemp))\n",
    "\n",
    "        AI_low_PCE_low=dftemp[dftemp['PCE_ECG_class']==1]\n",
    "        AI_low_PCE_low=AI_low_PCE_low[AI_low_PCE_low['PCEclass']==1]\n",
    "        AI_low_PCE_mod=dftemp[dftemp['PCE_ECG_class']==1]\n",
    "        AI_low_PCE_mod=AI_low_PCE_mod[AI_low_PCE_mod['PCEclass']==2]\n",
    "        AI_hig_PCE_low=dftemp[dftemp['PCE_ECG_class']==2]\n",
    "        AI_hig_PCE_low=AI_hig_PCE_low[AI_hig_PCE_low['PCEclass']==1]\n",
    "        AI_hig_PCE_mod=dftemp[dftemp['PCE_ECG_class']==2]\n",
    "        AI_hig_PCE_mod=AI_hig_PCE_mod[AI_hig_PCE_mod['PCEclass']==2]\n",
    "        \n",
    "        totalsum=len(AI_low_PCE_low)+len(AI_low_PCE_mod)+len(AI_hig_PCE_low)+len(AI_hig_PCE_mod)\n",
    "        \n",
    "        print('PCEECG_low_PCE_low',len(AI_low_PCE_low))\n",
    "        print('PCEECG_low_PCE_mod',len(AI_low_PCE_mod),len(AI_low_PCE_mod)/totalsum)\n",
    "        print('PCEECG_hig_PCE_low',len(AI_hig_PCE_low),len(AI_hig_PCE_low)/totalsum)\n",
    "        print('PCEECG_hig_PCE_mod',len(AI_hig_PCE_mod))\n",
    "        \n",
    "        print('totalsum',totalsum)\n",
    "        if i==1:\n",
    "            print('positive NRI',(len(AI_hig_PCE_low)-len(AI_low_PCE_mod))/totalsum)\n",
    "            NRI+=(len(AI_hig_PCE_low)-len(AI_low_PCE_mod))/totalsum\n",
    "        else:\n",
    "            print('negative NRI',(len(AI_low_PCE_mod)-len(AI_hig_PCE_low))/totalsum)\n",
    "            NRI+=(len(AI_low_PCE_mod)-len(AI_hig_PCE_low))/totalsum\n",
    "\n",
    "    print(NRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NRIdf_columns=[]\n",
    "NRIdf_columns.append('neg_PCElow_ECGmod')\n",
    "NRIdf_columns.append('neg_PCEmod_ECGlow')\n",
    "NRIdf_columns.append('neg_NRI')\n",
    "NRIdf_columns.append('pos_PCElow_ECGmod')\n",
    "NRIdf_columns.append('pos_PCEmod_ECGlow')\n",
    "NRIdf_columns.append('pos_NRI')\n",
    "NRIdf_columns.append('total_NRI')\n",
    "\n",
    "NRIdf=pd.DataFrame(columns=NRIdf_columns)\n",
    "\n",
    "for jjj in range(2000):\n",
    "    df_final_temp=df_final.sample(frac=1, replace=True)\n",
    "    \n",
    "    templist=[]\n",
    "    NRI=0\n",
    "    for i in [0,1]:\n",
    "        dftemp=df_final_temp[df_final_temp[statusname]==i]\n",
    "        #print('total',len(dftemp))\n",
    "\n",
    "        AI_low_PCE_low=dftemp[dftemp['PCE_ECG_class']==1]\n",
    "        AI_low_PCE_low=AI_low_PCE_low[AI_low_PCE_low['PCEclass']==1]\n",
    "        AI_low_PCE_mod=dftemp[dftemp['PCE_ECG_class']==1]\n",
    "        AI_low_PCE_mod=AI_low_PCE_mod[AI_low_PCE_mod['PCEclass']==2]\n",
    "        AI_hig_PCE_low=dftemp[dftemp['PCE_ECG_class']==2]\n",
    "        AI_hig_PCE_low=AI_hig_PCE_low[AI_hig_PCE_low['PCEclass']==1]\n",
    "        AI_hig_PCE_mod=dftemp[dftemp['PCE_ECG_class']==2]\n",
    "        AI_hig_PCE_mod=AI_hig_PCE_mod[AI_hig_PCE_mod['PCEclass']==2]\n",
    "\n",
    "        totalsum=len(AI_low_PCE_low)+len(AI_low_PCE_mod)+len(AI_hig_PCE_low)+len(AI_hig_PCE_mod)\n",
    "\n",
    "\n",
    "        #print('totalsum',totalsum)\n",
    "        if i==1:\n",
    "            NRI+=(len(AI_hig_PCE_low)-len(AI_low_PCE_mod))/totalsum\n",
    "        else:\n",
    "            NRI+=(len(AI_low_PCE_mod)-len(AI_hig_PCE_low))/totalsum\n",
    "        \n",
    "        templist.append(len(AI_hig_PCE_low)/totalsum)\n",
    "        templist.append(len(AI_low_PCE_mod)/totalsum)\n",
    "        if i==0:\n",
    "            templist.append((len(AI_low_PCE_mod)-len(AI_hig_PCE_low))/totalsum)\n",
    "        else:\n",
    "            templist.append((len(AI_hig_PCE_low)-len(AI_low_PCE_mod))/totalsum)\n",
    "        \n",
    "    templist.append(NRI)\n",
    "    #print(NRI)\n",
    "    NRIdf.loc[len(NRIdf)]=templist\n",
    "    if jjj%100==0:\n",
    "        print(jjj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NRIdfcols=list(NRIdf.columns)\n",
    "for col in NRIdfcols:\n",
    "    print(col)\n",
    "    print(np.percentile(NRIdf[col],97.5))\n",
    "    print(np.percentile(NRIdf[col],2.5))\n",
    "    print('======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concordance_index(df_final['day_difference'], -df_final['PCE'], df_final['status']))\n",
    "print(concordance_index(df_final['day_difference'], -df_final['PCE_AI'], df_final['status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_list=[]\n",
    "for iiii in range(2000):\n",
    "    df_final_resample=df_final.sample(frac=1, replace=True)\n",
    "\n",
    "    cindex_cvd_events_pce=concordance_index(df_final_resample['day_difference'], -df_final_resample['PCE'], df_final_resample['status'])\n",
    "    cindex_cvd_events_pce_ai=concordance_index(df_final_resample['day_difference'], -df_final_resample['PCE_AI'], df_final_resample['status'])\n",
    "\n",
    "\n",
    "    row = [\n",
    "        cindex_cvd_events_pce,\n",
    "        cindex_cvd_events_pce_ai\n",
    "    ]\n",
    "    rows_list.append(row)\n",
    "    if iiii%100==0:\n",
    "        print(iiii)\n",
    "df_C_INDEX = pd.DataFrame(rows_list, columns=['cindex_cvd_events_pce','cindex_cvd_events_pce_ai'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C_INDEX['cindex_cvd_events_diff']=df_C_INDEX['cindex_cvd_events_pce_ai']-df_C_INDEX['cindex_cvd_events_pce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24748a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_value_low = np.percentile(df_C_INDEX['cindex_cvd_events_pce'], 2.5)\n",
    "percentile_value_high = np.percentile(df_C_INDEX['cindex_cvd_events_pce'], 97.5)\n",
    "print(percentile_value_low,percentile_value_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_value_low = np.percentile(df_C_INDEX['cindex_cvd_events_pce_ai'], 2.5)\n",
    "percentile_value_high = np.percentile(df_C_INDEX['cindex_cvd_events_pce_ai'], 97.5)\n",
    "print(percentile_value_low,percentile_value_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1945f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_value_low = np.percentile(df_C_INDEX['cindex_cvd_events_diff'], 2.5)\n",
    "percentile_value_high = np.percentile(df_C_INDEX['cindex_cvd_events_diff'], 97.5)\n",
    "print(percentile_value_low,percentile_value_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaed53",
   "metadata": {},
   "source": [
    "# characteristics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIcat=[]\n",
    "for i in range(len(df)):\n",
    "    if df['AI_prediction_average'][i]>=thres2:\n",
    "        AIcat.append(3)\n",
    "    elif df['AI_prediction_average'][i]>=thres1:\n",
    "        AIcat.append(2)\n",
    "    else:\n",
    "        AIcat.append(1)\n",
    "df['AIcat']=AIcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "statcols=[]\n",
    "statcols.append('year_difference')\n",
    "statcols.append('sexint')\n",
    "statcols.append('DM')\n",
    "statcols.append('HTN')\n",
    "statcols.append('smoking')\n",
    "statcols.append('SBP')\n",
    "statcols.append('Chol')\n",
    "statcols.append('HDL')\n",
    "statcols.append('AI_prediction_average')\n",
    "statcols.append('PCE')\n",
    "statcols.append('status')\n",
    "statcols.append('AIcat')\n",
    "\n",
    "\n",
    "catcols=[]\n",
    "catcols.append('sexint')\n",
    "catcols.append('DM')\n",
    "catcols.append('HTN')\n",
    "catcols.append('smoking')\n",
    "catcols.append('status')\n",
    "catcols.append('AIcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats=df[statcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0628b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table1(catcolumn,catcolumns,dfdfdf2):\n",
    "    nonnormallist=[]\n",
    "    def _normality(self, x):\n",
    "        #print(x.name)\n",
    "\n",
    "        if len(x.values[~np.isnan(x.values)]) >= 20:\n",
    "            p = stats.shapiro(x.values).pvalue\n",
    "        else:\n",
    "            p = None\n",
    "        # dropna=False argument in pivot_table does not function as expected\n",
    "        # return -1 instead of None\n",
    "        if pd.isnull(p):\n",
    "            return -1\n",
    "        if p<=0.05:\n",
    "            nonnormallist.append(x.name)\n",
    "        return p\n",
    "\n",
    "    TableOne._normality=_normality\n",
    "\n",
    "    def my_custom_test(group1, group2):\n",
    "        \"\"\"\n",
    "        Hypothesis test for test_self_defined_statistical_tests\n",
    "        \"\"\"\n",
    "        my_custom_test.__name__ = \"mannwhitneyu\"\n",
    "        _, pval= scipy.stats.mannwhitneyu(group1, group2)\n",
    "        return pval\n",
    "\n",
    "    nonnormallist=[]\n",
    "    def _normality(self, x):\n",
    "        #print(x.name)\n",
    "\n",
    "        if len(x.values[~np.isnan(x.values)]) >= 20:\n",
    "            p = stats.shapiro(x.values).pvalue\n",
    "        else:\n",
    "            p = None\n",
    "        # dropna=False argument in pivot_table does not function as expected\n",
    "        # return -1 instead of None\n",
    "        if pd.isnull(p):\n",
    "            return -1\n",
    "        if p<=0.05:\n",
    "            nonnormallist.append(x.name)\n",
    "        return p\n",
    "\n",
    "    TableOne._normality=_normality\n",
    "\n",
    "    table1=TableOne(dfdfdf2,categorical=catcolumns,groupby=[catcolumn],normal_test=True,pval=True,htest_name=True,decimals=3)\n",
    "    nonnormallist=list(set(nonnormallist))\n",
    "    nonnormallist\n",
    "\n",
    "\n",
    "    table1=TableOne(dfdfdf2,categorical=catcolumns,groupby=[catcolumn],normal_test=True,pval=True,htest_name=True,nonnormal=nonnormallist,decimals=3)\n",
    "    try:\n",
    "        os.mkdir(newtablename)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(newtablename+'/table1')\n",
    "    except:\n",
    "        pass\n",
    "    catcolumn=catcolumn.replace(' ','_')\n",
    "    catcolumn=catcolumn.replace('/','_')\n",
    "    table1.to_html('figures/20241001_SH_table1_'+catcolumn+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4333d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1('AIcat',catcols,df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7667446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx4090",
   "language": "python",
   "name": "rtx4090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

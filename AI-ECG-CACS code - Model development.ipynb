{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd36e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import base64\n",
    "import array\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from torchsummary import summary\n",
    "from tableone import TableOne\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bdd01",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8de5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels * r),\n",
    "            Swish(),\n",
    "            nn.Linear(in_channels * r, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.excitation(x)\n",
    "        x = x.view(x.size(0), x.size(1), 1)\n",
    "        return x\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    expand = 6\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first MBConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels * MBConv.expand, 1, stride=stride, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish(),\n",
    "            nn.Conv1d(in_channels * MBConv.expand, in_channels * MBConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*MBConv.expand),\n",
    "            nn.BatchNorm1d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * MBConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv1d(in_channels*MBConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "class SepConv(nn.Module):\n",
    "    expand = 1\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first SepConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels * SepConv.expand, in_channels * SepConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*SepConv.expand),\n",
    "            nn.BatchNorm1d(in_channels * SepConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * SepConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv1d(in_channels*SepConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n",
    "        super().__init__()\n",
    "        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "        repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
    "        depth = depth_coef\n",
    "        width = width_coef\n",
    "\n",
    "        channels = [int(x*width) for x in channels]\n",
    "        repeats = [int(x*depth) for x in repeats]\n",
    "\n",
    "        # stochastic depth\n",
    "        if stochastic_depth:\n",
    "            self.p = p\n",
    "            self.step = (1 - 0.5) / (sum(repeats) - 1)\n",
    "        else:\n",
    "            self.p = 1\n",
    "            self.step = 0\n",
    "\n",
    "\n",
    "        # efficient net\n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode='linear', align_corners=False)\n",
    "\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv1d(8, channels[0],3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(channels[0], momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n",
    "\n",
    "        self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n",
    "\n",
    "        self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n",
    "\n",
    "        self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n",
    "\n",
    "        self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n",
    "\n",
    "        self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n",
    "\n",
    "        self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n",
    "\n",
    "        self.stage9 = nn.Sequential(\n",
    "            nn.Conv1d(channels[7], channels[8], 1, stride=1, bias=False),\n",
    "            nn.BatchNorm1d(channels[8], momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        ) \n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(channels[8], num_classes)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.stage8(x)\n",
    "        x = self.stage9(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        xsoft=self.softmax(x)\n",
    "        return x,xsoft\n",
    "\n",
    "\n",
    "    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n",
    "        strides = [stride] + [1] * (repeats - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n",
    "            in_channels = out_channels\n",
    "            self.p -= self.step\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def efficientnet_b0(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b1(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b2(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b3(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b4(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b5(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b6(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "def efficientnet_b7(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir='' # enter file path here\n",
    "wf_total=torch.load(filedir+'.pt') # enter the name of the waveform file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(bs,lr,weight_decay,epoch_num,optimizer_type,label,trainvaltest_iter,themodel,modelname):\n",
    "    \n",
    "    df_merged3=pd.read_csv('.csv') # enter the name of the data file here\n",
    "    print('len df_merged3',len(df_merged3))\n",
    "\n",
    "    index_test=[]\n",
    "    index_trainval=[]\n",
    "    for i in range(len(df_merged3)):\n",
    "        if '체크업' in df_merged3['처방코드(코드명)'][i]: # Korean term for checkup\n",
    "            index_test.append(i)\n",
    "        else:\n",
    "            index_trainval.append(i)\n",
    "\n",
    "    df_test_temp=df_merged3.iloc[index_test]\n",
    "    df_test_temp.reset_index(inplace=True,drop=True)\n",
    "    df_test_temp=df_test_temp[df_test_temp['datediff_int']<=0]\n",
    "    df_test_temp=df_test_temp[df_test_temp['datediff_int']>=-0]\n",
    "    df_test_temp.reset_index(inplace=True,drop=True)\n",
    "    print('test length',len(df_test_temp))\n",
    "    print('test >= 100:',len(df_test_temp[df_test_temp['CACS']>=100]))\n",
    "    print('test >= 400:',len(df_test_temp[df_test_temp['CACS']>=400]))\n",
    "    print('test >= 1000:',len(df_test_temp[df_test_temp['CACS']>=1000]))\n",
    "\n",
    "    df_test=df_merged3.iloc[index_test]\n",
    "    df_trainval=df_merged3.iloc[index_trainval]\n",
    "\n",
    "    df_test.reset_index(inplace=True,drop=True)\n",
    "    df_trainval.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    df_test=df_test[df_test['datediff_int']<=0]\n",
    "    df_test=df_test[df_test['datediff_int']>=-0]\n",
    "    df_test.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "    labels_temp = df_trainval[label]\n",
    "    groups_temp = df_trainval['patient_id']\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(df_trainval, labels_temp, groups_temp):\n",
    "        df_train = df_trainval.iloc[train_idx].reset_index(drop=True)\n",
    "        df_val = df_trainval.iloc[val_idx].reset_index(drop=True)\n",
    "        break  \n",
    "\n",
    "    df_train.reset_index(inplace=True,drop=True)\n",
    "    df_val.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    print('len df_train',len(df_train))\n",
    "    print('len df_val',len(df_val))\n",
    "    print('len df_test',len(df_test))\n",
    "    print('len df_test positive',np.sum(df_test[label]))\n",
    "    \n",
    "    savepath=''# enter savepath here\n",
    "    \n",
    "    df_train.to_csv(savepath+'df_train.csv')\n",
    "    df_val.to_csv(savepath+'df_val.csv')\n",
    "    df_test.to_csv(savepath+'df_test.csv')\n",
    "    \n",
    "    class Agatston_train(Dataset):\n",
    "        def __init__(self):\n",
    "            self.thedf=df_train\n",
    "            self.len=len(self.thedf)\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "            self.primary_key=self.thedf['primary_key'][index]\n",
    "            randomint=random.randint(0,3750)\n",
    "            self.input=wf_total[self.primary_key][:,randomint:randomint+1250]\n",
    "            self.output=self.thedf[label][index]\n",
    "            return self.input,self.output\n",
    "\n",
    "        def get_labels(self): \n",
    "            return np.array(self.thedf[label])\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "\n",
    "    class Agatston_val(Dataset):\n",
    "        def __init__(self):\n",
    "            self.thedf=df_val\n",
    "            self.len=len(self.thedf)*4\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "            index_a=index//4\n",
    "            index_b=index%4\n",
    "            self.primary_key=self.thedf['primary_key'][index_a]\n",
    "            self.input=wf_total[self.primary_key][:,index_b*1250:(index_b+1)*1250]\n",
    "            self.output=self.thedf[label][index_a]\n",
    "            return self.input,self.output\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "\n",
    "    class Agatston_test(Dataset):\n",
    "        def __init__(self):\n",
    "            self.thedf=df_test\n",
    "            self.len=len(self.thedf)*4\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "            index_a=index//4\n",
    "            index_b=index%4\n",
    "            self.primary_key=self.thedf['primary_key'][index_a]\n",
    "            self.input=wf_total[self.primary_key][:,index_b*1250:(index_b+1)*1250]\n",
    "            self.output=self.thedf[label][index_a]\n",
    "            return self.input,self.output\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "    \n",
    "\n",
    "    train_dataset = Agatston_train()\n",
    "    val_dataset = Agatston_val()\n",
    "    test_dataset = Agatston_test()\n",
    "\n",
    "    train_dataset_dataloader = DataLoader(train_dataset,batch_size=bs, sampler=ImbalancedDatasetSampler(train_dataset))\n",
    "    val_dataset_dataloader = DataLoader(val_dataset,batch_size=bs)\n",
    "    test_dataset_dataloader = DataLoader(test_dataset,batch_size=bs)\n",
    "    \n",
    "\n",
    "    model=themodel\n",
    "    criterion=nn.CrossEntropyLoss().to(device)\n",
    "    if optimizer_type=='Adam':\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "    elif optimizer_type=='RMSprop':\n",
    "        optimizer=torch.optim.RMSprop(model.parameters(),lr=lr, weight_decay=weight_decay,alpha=0.9, momentum=0.1)\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=5,factor=0.5,mode='min')\n",
    "\n",
    "    \n",
    "    toggle=0\n",
    "    accum=0\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        now1=datetime.datetime.now()\n",
    "        \n",
    "        if toggle==1:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        for phase in trainvaltest_iter:\n",
    "            if phase=='training':\n",
    "                running_loss=0.0\n",
    "                acc=0.\n",
    "                correct=0\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(param_group['lr']) \n",
    "                TP=0\n",
    "                TN=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                model.train()\n",
    "                for i,data in enumerate(train_dataset_dataloader):\n",
    "                    inputs,targets=data\n",
    "                    inputs=inputs.to(device)\n",
    "                    targets=targets.long().to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs,outputssoft=model(inputs)\n",
    "                    loss=criterion(outputs,targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss+=loss.item()\n",
    "                    prediction=torch.max(outputs.data,1)[1]\n",
    "                    correct+=prediction.eq(targets.data.view_as(prediction)).cpu().sum()\n",
    "                    for a,b in zip(targets,prediction):\n",
    "                        if a==1 and b==1:\n",
    "                            TP+=1\n",
    "                        elif a==1 and b==0:\n",
    "                            FN+=1\n",
    "                        elif a==0 and b==1:\n",
    "                            FP+=1\n",
    "                        else:\n",
    "                            TN+=1 \n",
    "                sensitivity=(TP/(TP+FN))\n",
    "                specificity=(TN/(TN+FP))\n",
    "                try:\n",
    "                    precision=(TP/(TP+FP))\n",
    "                except:\n",
    "                    precision=0\n",
    "                if precision!=0:\n",
    "                    f1score=((2*precision*sensitivity)/(precision+sensitivity))\n",
    "                else:\n",
    "                    f1score=0\n",
    "                print(\"[%d] loss: %.3f  sensitivity: %.3f specificity: %.3f precision: %.3f f1score: %.3f\" % (epoch+1,running_loss/100,sensitivity,specificity,precision,f1score))     \n",
    "\n",
    "\n",
    "            if phase=='val':\n",
    "                model.eval()\n",
    "                running_loss=0.0\n",
    "                acc=0.\n",
    "                correct=0\n",
    "                TP=0\n",
    "                TN=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                prediction_score=[]\n",
    "                targets_list=[]\n",
    "                with torch.no_grad():\n",
    "                    for i,data in enumerate(val_dataset_dataloader):\n",
    "                        inputs,targets=data\n",
    "                        for j in range(len(targets)):\n",
    "                            targets_list.append(targets[j])\n",
    "                        inputs=inputs.to(device)\n",
    "                        targets=targets.long().to(device)\n",
    "                        outputs,outputssoft=model(inputs)\n",
    "                        loss=criterion(outputs,targets)\n",
    "                        running_loss+=loss.item()\n",
    "                        prediction=torch.max(outputs.data,1)[1]\n",
    "                        correct+=prediction.eq(targets.data.view_as(prediction)).cpu().sum()\n",
    "\n",
    "                        for a,b in zip(targets,prediction):\n",
    "                            if a==1 and b==1:\n",
    "                                TP+=1\n",
    "                            elif a==1 and b==0:\n",
    "                                FN+=1\n",
    "                            elif a==0 and b==1:\n",
    "                                FP+=1\n",
    "                            else:\n",
    "                                TN+=1\n",
    "                        for j in range(len(outputssoft)):\n",
    "                            prediction_score.append(outputssoft[j][1].item())\n",
    "\n",
    "                try:\n",
    "                    sensitivity=(TP/(TP+FN))\n",
    "                except:\n",
    "                    sensitivity=0.5\n",
    "                try:\n",
    "                    specificity=(TN/(TN+FP))\n",
    "                except:\n",
    "                    specificity=0.5\n",
    "                try:\n",
    "                    precision=(TP/(TP+FP))\n",
    "                except:\n",
    "                    precision=0\n",
    "                if precision!=0:\n",
    "                    f1score=((2*precision*sensitivity)/(precision+sensitivity))\n",
    "                else:\n",
    "                    f1score=0                \n",
    "\n",
    "                fpr_temp,tpr_temp,threshold=metrics.roc_curve(targets_list,prediction_score,pos_label=1)\n",
    "                AUROC_val=metrics.auc(fpr_temp,tpr_temp)\n",
    "                AP=average_precision_score(targets_list,prediction_score)\n",
    "\n",
    "                print(\"[%d] loss: %.3f AUROC: %.3f AUPRC: %.3f sensitivity: %.3f specificity: %.3f precision: %.3f f1score: %.3f\" % (epoch+1,running_loss/100,AUROC_val,AP, sensitivity,specificity,precision,f1score))     \n",
    "                scheduler.step(running_loss)\n",
    "                \n",
    "                if epoch==0:\n",
    "                    lowest_val_loss=running_loss\n",
    "                else:\n",
    "                    if running_loss<lowest_val_loss:\n",
    "                        lowest_val_loss=running_loss\n",
    "                        accum=0\n",
    "                    else:\n",
    "                        accum+=1\n",
    "                    if accum==15:\n",
    "                        toggle=1\n",
    "                \n",
    "            if phase=='test':\n",
    "                model.eval()\n",
    "                running_loss=0.0\n",
    "                acc=0.\n",
    "                correct=0\n",
    "                TP=0\n",
    "                TN=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                prediction_score=[]\n",
    "                targets_list=[]\n",
    "                with torch.no_grad():\n",
    "                    for i,data in enumerate(test_dataset_dataloader):\n",
    "                        inputs,targets=data\n",
    "                        for j in range(len(targets)):\n",
    "                            targets_list.append(targets[j])\n",
    "                        inputs=inputs.to(device)\n",
    "                        targets=targets.long().to(device)\n",
    "                        outputs,outputssoft=model(inputs)\n",
    "                        loss=criterion(outputs,targets)\n",
    "                        running_loss+=loss.item()\n",
    "                        prediction=torch.max(outputs.data,1)[1]\n",
    "                        correct+=prediction.eq(targets.data.view_as(prediction)).cpu().sum()\n",
    "\n",
    "                        for a,b in zip(targets,prediction):\n",
    "                            if a==1 and b==1:\n",
    "                                TP+=1\n",
    "                            elif a==1 and b==0:\n",
    "                                FN+=1\n",
    "                            elif a==0 and b==1:\n",
    "                                FP+=1\n",
    "                            else:\n",
    "                                TN+=1\n",
    "                        for j in range(len(outputssoft)):\n",
    "                            prediction_score.append(outputssoft[j][1].item())\n",
    "\n",
    "                try:\n",
    "                    sensitivity=(TP/(TP+FN))\n",
    "                except:\n",
    "                    sensitivity=0.5\n",
    "                try:\n",
    "                    specificity=(TN/(TN+FP))\n",
    "                except:\n",
    "                    specificity=0.5\n",
    "                try:\n",
    "                    precision=(TP/(TP+FP))\n",
    "                except:\n",
    "                    precision=0\n",
    "                if precision!=0:\n",
    "                    f1score=((2*precision*sensitivity)/(precision+sensitivity))\n",
    "                else:\n",
    "                    f1score=0                \n",
    "                \n",
    "                fpr_temp,tpr_temp,threshold=metrics.roc_curve(targets_list,prediction_score,pos_label=1)\n",
    "                AUROC=metrics.auc(fpr_temp,tpr_temp)\n",
    "                AP=average_precision_score(targets_list,prediction_score)\n",
    "\n",
    "                print(\"[%d] loss: %.3f AUROC: %.3f AUPRC: %.3f sensitivity: %.3f specificity: %.3f precision: %.3f f1score: %.3f\" % (epoch+1,running_loss/100,AUROC,AP, sensitivity,specificity,precision,f1score))     \n",
    " \n",
    "                now = datetime.datetime.now()\n",
    "                nowStr2 = \"{:%Y%m%d%H%M%S}\".format(now)\n",
    "                modelname2=nowStr2+'_'\n",
    "                modelname2+=(label+'_')\n",
    "                modelname2+=modelname\n",
    "                modelname2+=('_'+'epoch_'+str(epoch))\n",
    "                modelname2+='.pt'\n",
    "\n",
    "                torch.save(model.state_dict(),savepath+modelname2)\n",
    "\n",
    "        now2=datetime.datetime.now()\n",
    "        print(now2-now1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases=['training','val','test']\n",
    "experimentnum=0\n",
    "for model in [efficientnetb0]:\n",
    "    for batchsize in [128,256,512,1024]:\n",
    "        for learningrate in [0.001,0.01,0.1]:\n",
    "            for weightdecay in [0,0.001,0.01]:\n",
    "                print('===================================================')\n",
    "                print(experimentnum,model,batchsize,learningrate,weightdecay)\n",
    "                themodel=model().to(device)\n",
    "                modelname='experimentnum'\n",
    "                modelname+=str(experimentnum)\n",
    "                modelname+='_'\n",
    "                modelname+=(str(model).split('function')[1][1:].split('at')[0][:-1]+'_')\n",
    "                pipeline(batchsize,learningrate,weightdecay,50,'Adam',\"label0\",phases,themodel,modelname)\n",
    "                experimentnum+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7667446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx4090",
   "language": "python",
   "name": "rtx4090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
